#+TITLE:     Pattern Recogniton and Machine Learning(1)
#+AUTHOR:    mingzailao
#+EMAIL:     mingzailao@gmail.com
#+DATE:      Sat Mar  4 21:11:13 2017
#+DESCRIPTION: 
#+KEYWORDS: 
#+STARTUP: beamer
#+STARTUP: oddeven
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+BEAMER_THEME: metropolis
#+OPTIONS:   H:2 toc:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)
#+LATEX_HEADER:\def\mathfamilydefault{\rmdefault}
#+BEGIN_EXPORT latex
\AtBeginSection[]
{
\begin{frame}<beamer>
\frametitle{}
\tableofcontents[currentsection]
\end{frame}
}
#+END_EXPORT






* Mathematical notation

** Mathematical notation

1. $\mathbf{x}$ : vector(column vector)
2. $\mathbf{x}^T$ : row vector
3. $\mathbf{M}$ : Matrix
4. $(\omega_{1},\omega_2,\cdots,\omega_{M})$ : row vector with $M$ elements
5. $\mathbf{w}$ : $(\omega_1,\omega_2,\cdots,\omega_{M})^T$
6. $\mathbf{I}_M$ : the $M\times M$ identity matrix
7. $f[y]$ : $y(x)$ in some function( this is a functional concept)
8. $g(x)=\mathem{O}(f(x))$ denotes that $|f(x)/g(x)|$ is bounded as $x\rightarrow     \infty$.($g(x)=3x^{2}+2$ then $g(x)=\mathem{O}(x^2)$)
9. $\mathbb{E}_{x\sim \mathbf{x}}[f(x,y)]$ : the expection of a function $f(x,y)$     with respect to a random variable $\mathbf{x}$





* An example

** Polynomial Curve Fitting

*** Discribe
1) $N$ observations of $x$ : $\mathbf{X}=(x_1,\cdots,x_N)^N$;
2) $N$ observations of $y$ : $\mathbf{Y}=(t_1,\cdots,t_N)^N$;

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 16:38:21
[[file:Introduction/screenshot_2016-12-04_16-38-21.png]]

** Polynomial Curve Fitting

*** Goal

Polynomial function Space with parameters $\mathbf{w}$:
\begin{equation*}
\label{eq:1}
y(x,\mathbf{w})=\omega_0+\omega_1x+\cdots+\omega_Mx^M=\sum_{j=0}^M\omega_jx^j
\end{equation*}
Find a polynomial function $y(x,\mathbf{w})$ to minimize:
\begin{equation*}
\label{eq:2}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^N\{y(x_n,\mathbf{w})-t_n\}^2
\end{equation*}
** Polynomial Curve Fitting

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 17:04:49
[[file:Introduction/screenshot_2016-12-04_17-04-49.png]]

** Polynomial Curve Fitting


#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 20:40:49
[[file:Introduction/screenshot_2016-12-04_20-40-49.png]]
** Polynomial Curve Fitting
*** How to chose $M$
    For each choice of $M$, we can then evaluate the residual value of 
    $E(\mathbf{w}^{*})$
    given by the equation for the training data, and we can also evaluate 
    $E(\mathbf{w}^{*})$ for the training data, use RMS:
    
\begin{equation*}
\label{eq:3}
E_{RMS}=\sqrt{2E(\mathbf{w}^{*})/N}
\end{equation*}
** How to chose $M$

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 20:42:58
[[file:Introduction/screenshot_2016-12-04_20-42-58.png]]


** Regularization
*** Mathematical Equation
\begin{equation*}
\label{eq:4}
\tilde{E}(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2+\frac{\lambda}{2}||\mathbf{w}||_2^2
\end{equation*}


#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 20:51:00
[[file:Introduction/screenshot_2016-12-04_20-51-00.png]]

** My Notes:
1. The equation (4) has close form
2. In the statistics, called "ridge regression"
3. In neural networks, called "weight decay"

** Regularization

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 20:55:31
[[file:Introduction/screenshot_2016-12-04_20-55-31.png]]


* Probability Theory(only some important concepts)
** The Rules of Probability
*** Rules 
1. sum rule : $$p(X)=\sum_Y P(X,Y)$$
2. product rule : $$P(X,Y)=P(Y|X)P(X)$$
*** Bayes
\begin{equation*}
\label{eq:5}
P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}
\end{equation*}
** Probability densities
*** Definition
If the probability of a real-valued variable x falling in the interval 
$(x,x+\delta x)$ is given by $p(x)\delta x$ for $\delta x\rightarrow 0$


#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-04 21:06:09
[[file:Probability Theory(only some important concepts)/screenshot_2016-12-04_21-06-09.png]]

** Probability densities
*** Property
1. $$P(x)\ge 0$$
2. $$\int_{- \infty}^{\infty}p(x)dx=1$$

** Multi-variable case
*** Property
1. $$p(\mathbf{x})\ge 0$$
2. $$\int p(\mathbf{x})d\mathbf{x}=1$$
** Sum rule and product rule
1. $$p(x)=\int p(x,y)dy$$
2. $$p(x,y)=p(y|x)p(x)$$
** Expectations and covariances
*** Expectation
\begin{equation*}
\label{eq:6}
\mathbb{E}[f]=\sum_{x}p(x)f(x)
\end{equation*}
\begin{equation*}
\label{eq:7}
\mathbb{E}[f]=\int p(x)f(x)dx
\end{equation*}
*** Notes
In practice, we can only get a finite number N of points drawn from the probability
distribution or probability density, we can use :
\begin{equation}
\label{eq:8}
\mathbb{E}[f]\approx\frac{1}{N}\sum_{n=1}^N f(x_n)
\end{equation}
** Expectations and covariances
*** Conditional expectation
\begin{equation*}
\label{eq:9}
\mathbb{E}_{x}[f(x|y)]=\sum_x p(x|y)f(x)
\end{equation*}
\begin{equation*}
\label{eq:10}
\mathbb{E}_{x}[f(x|y)]=\int_{x}p(x|y)f(x)dx
\end{equation*}

** Expectations and covariances
*** Covariances

\begin{equation*}
\label{eq:11}
var[f]=\mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2
\end{equation*}

*** Notes
In particular, we can consider the variance of the variable x itself,
which is given by
\begin{equation*}
\label{eq:12}
var[x]=\mathbb{E}[x^2]-\mathbb{E}_[x]^{2}
\end{equation*}
** Expectations and covariances
*** Covariance
\begin{equation}
\label{eq:13}
cov[x,y]=\mathbb{E}_{x,y}[\{x-\mathbb{E}[x]]\}\{y-\mathbb{E}[y]\}=\mathbb{E}_{x,y}[xy]-\mathbb{E}[x]\mathbb{E}[y]
\end{equation}
*** Covariance matrix
\begin{equation*}
\label{eq:14}
cov[\mathbf{x},\mathbf{y}]=\mathbb{E}_{\mathbf{x},\mathbf{y}}[\mathbf{x}\mathbf{y}^T]-\mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{y}^T]
\end{equation*}
*** Notes 
- $cov[\mathbf{x}]=cov[\mathbf{x},\mathbf{x}]$
*** Question
what $cov[\mathbf{x}]=diag$ means?

** The Gaussian distribution
*** The Gaussian distribution
\begin{equation*}
\label{eq:15}
\mathcal{N}(x|\mu,\sigma^2)=\frac{1}{(2\pi \sigma^2)}\exp \{\frac{1}{2\sigma^2}(x-\mu)^2\}
\end{equation*}

\begin{equation}
\label{eq:17}
\mathcal{N}(\mathbf{x}|\boldsymbol{\mu},\boldsymbol{\Sigma})=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\boldsymbol{\Sigma}|^{1/2}}\exp \{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\}
\end{equation}

** The Gaussian distribution
*** For a dataset
for a dataset $\mathbf{X}=(x_1,\cdots,x_N)^T$,
\begin{equation*}
\label{eq:18}
p(\mathbf{X}|\mathbf{\mu},\Sigma)=\prod_{n=1}^N \mathbf{N}(x_n|\mu,\sigma^2)
\end{equation*}

*** The log-likelihood
\begin{equation}
\label{eq:19}
\ln p(\mathbf{X}|\mu,\sigma^{2})=-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2-\frac{N}{2}\ln \sigma^2-\frac{N}{2}\ln(2\pi)
\end{equation}
** The Gaussian distribution
*** Maximizing with respect to $\mu$ and $\sigma^2$
\begin{equation*}
\label{eq:20}
\mu_{ML}=\frac{1}{N}\sum_{n=1}^Nx_n
\end{equation*}

\begin{equation*}
\label{eq:21}
\sigma^2_{ML}=\frac{1}{N}\sum_{n=1}^N(x_n-\mu_{ML})^2
\end{equation*}

and 
\begin{equation*}
\label{eq:22}
\mathbb{E}[\mu_{ML}]=\mu
\end{equation*}
\begin{equation*}
\label{eq:23}
\mathbb{E}[\sigma_{ML}^2]=(\frac{N-1}{N})\sigma^2
\end{equation*}

** Back to the example
*** Rethink
Given $x$, the corresponding value of $t$ has a Gaussian distribution with a mean 
value $y(x,\mathbf{w})$, thus:
\begin{equation*}
\label{eq:16}
p(t|x,\mathbf{w},\beta)=\mathcal{N}(t|y(x;\mathbf{w}),\beta^{-1})
\end{equation*}

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-05 11:29:36

[[file:Probability Theory(only some important concepts)/screenshot_2016-12-05_11-29-36.png]]

** Back to example
*** Rethink
\begin{equation*}
p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta)=\prod_{n=1}^N\mathcal{N}(t_n|y(x_n,\mathbf{w}),\beta^{-1})
\end{equation*}
\begin{equation*}
\label{eq:26}
\ln p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta)=-\frac{\beta}{2}\sum_{n=1}^N\{y(x_n,\mathbf{w})-t_n\}+\frac{N}{2}\ln \beta-\frac{N}{2}\ln (2\pi)
\end{equation*}

\begin{equation*}
\label{eq:27}
\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^N\{y(x_n,\mathbf{w})-t_n\}^2
\end{equation*}

** More Bayes
*** Hyperparameters

\begin{equation}
\label{eq:28}
p(\mathbf{w}|\alpha)=\mathcal{N}(\mathbf{w}|0,\alpha^{-1}\mathbf{I})=(\frac{\alpha}{2\pi})^{(M+1)/2}\exp \{-\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}\}
\end{equation}
*** MAP
\begin{equation*}
\label{eq:30}
p(\mathbf{w}|\mathbf{X},\mathbf{t},\alpha,\beta)\propto p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta)p(\mathbf{w}|\alpha)
\end{equation*}
then, maximum of the posterior is given by the minimum of
\begin{equation}
\label{eq:24}
\frac{\beta}{2}\sum_{n=1}^N\{y(x_n,\mathbf{w})-t_n\}^2+\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}
\end{equation}
** Bayesian curve fitting
\begin{equation}
\label{eq:25}
p(t|x,\mathbf{x},\mathbf{t})=\mathcal{N}(t|m(x),s^2(x))
\end{equation}
where :
\begin{eqnarray}
\label{eq:32}
m(x) & = & \beta\boldsymbol{\phi}(x)^T\mathbf{S}\sum_{n=1}^{N}\boldsymbol{\phi}(x_n)t_n\\
s^2(x)&=& \beta^{-1}+\boldsymbol{\phi}(x)^T\mathbf{S}\boldsymbol{\phi}(x)
\end{eqnarray}
where 
\begin{equation}
\label{eq:33}
\mathbf{S}^{-1}=\alpha I+ \beta\sum_{n=1}^N\boldsymbol{\phi}(x_n)\boldsymbol{\phi}(x_n)^T
\end{equation}
** Decision Theory
Given :
- image $\mathbf{x}$ 
TODO :
- decide which of the two classes to assign to the image
\begin{equation}
\label{eq:34}
p(\mathcal{C}_k|\mathbf{x})=\frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}
{p(\mathbf{x})}
\end{equation}
*** Minimizing the misclassification rate
\begin{align}
\label{eq:36}
p(mistake)&=&p(\mathbf{x}\in \mathcal{R}_1,\mathcal{C}_2)+p(\mathbf{x}\in \mathcal{R}_2,\mathcal{C}_1)\\
&=& \int_{\mathcal{R}_1}p(\mathbf{x},\mathcal{C}_2)d\mathbf{x}+\int_{\mathcal{R}_{2}}p(\mathbf{x},\mathcal{C}_1)
\end{align}
** Information Theory
*** Entropy
\begin{equation}
\label{eq:37}
H(P)=\mathbb{E}_{\mathbf{x}\sim P}[-log \mathbf{x}]
\end{equation}

*** Maximizing it with three constraints
\begin{equation}
\label{eq:40}
\int_{-\infty}^{\infty}p(\mathbf{x})d\mathbf{x}=1
\end{equation}

\begin{equation}
\label{eq:41}
\int_{-\infty}^{\infty}\mathbf{x}p(\mathbf{x})d\mathbf{x}=\mu
\end{equation}
\begin{equation}
\label{eq:42}
\int_{-\infty}^{\infty}(\mathbf{x}-\mu)^2p(\mathbf{x})d\mathbf{x}=\sigma^2
\end{equation}
** Information Theory
The constrained maximization can be performed using Lagrange multipliers so that we maximize the following functional with respect to $p(\mathbf{x})$:
\begin{align}
\label{eq:45}
-&\int_{-\infty}^{\infty}p(\mathbf{x})\ln p(\mathbf{x})d\mathbf{x}+\lambda_1(\int_{-\infty}^{\infty}p(\mathbf{x})d\mathbf{x}-1)&\\
+&\lambda_2(\int_{-\infty}^{\infty}\mathbf{x}p(\mathbf{x})d\mathbf{x}-\mu)+\lambda_3(\int_{-\infty}^{\infty}(\mathbf{x}-\mu)^2p(\mathbf{x})d\mathbf{x}-\sigma^{2})
\end{align}
Using the calculus of variations, we set the derivative of this functional to zero giving
\begin{equation}
\label{eq:46}
p(\mathbf{x})=exp\{-1+\lambda_1+\lambda_{2}\mathbf{x}+\lambda_{3}(\mathbf{x}-\mu)^2\}
\end{equation}
** Information Theory
The Lagrange multipliers can be found by back substitution of this result into the three constraint equations, leading finally to the result
\begin{equation}
\label{eq:47}
p(\mathbf{x})=\frac{1}{(2\pi \sigma^2)^{1/2}}exp\{-\frac{(\mathbf{x}-\mu)^2}{2\sigma^2}\}
\end{equation}
*** The entropy if Gaussian
\begin{equation}
\label{eq:48}
H[\mathbf{x}]=\frac{1}{2}\{1+\ln (2\pi\sigma^2)\}
\end{equation}
** Information Theory
*** conditional entropy
    
\begin{equation}
\label{eq:49}
H[\mathbf{y}|\mathbf{x}]=-\int\int p(\mathbf{y},\mathbf{x})\ln p(\mathbf{y}|\mathbf{x})d\mathbf{y}d\mathbf{x}
\end{equation}

the conditional entropy satisfies the relation
\begin{equation}
\label{eq:50}
H[\mathbf{x},\mathbf{y}]=H[\mathbf{y}|\mathbf{x}]+H[\mathbf{x}]
\end{equation}
** Relative entropy and mutual information
*** Relative entropy or KL-divergence(Kullback-Leibler divergence)
\begin{equation}
\label{eq:29}
KL(p||q)=\int_{\mathcal{X}}p(\mathbf{x})\ln\frac{p(\mathbf{x})}{q(\mathbf{x})}d\mathbf{x}
\end{equation}
\begin{eqnarray*}
\label{eq:35}
KL(p||q)&=&\int_{\mathcal{X}} p(\mathbf{x})(-\ln \frac{q(\mathbf{x})}{p(\mathbf{x})})\\
&\ge&-\ln \int_{\mathcal{X}}q(\mathbf{x})d\mathbf{x}\\
&=&0
\end{eqnarray*}
** Relative entropy and mutual information
 for a unknown distribution $p(\mathbf{x})$, approximating this distribution using some parametric distribution $q(\mathbf{x}|\boldsymbol{\theta})$
\begin{equation}
\label{eq:38}
KL_{\boldsymbol{\theta}}(p||q)\approx \sum_{n=1}^N\{-\ln q(\mathbf{x}_n|\boldsymbol{\theta})+\ln p(\mathbf{x}_n)\}
\end{equation}
- $arg\min_{\boldsymbol{\theta}} KL_{\boldsymbol{\theta}}(p||q)\Leftrightarrow arg\max_{\boldsymbol{\theta}}likelihood$

** Relative entropy and mutual information
Now consider the joint distribution between two sets of variables $\mathbf{x},\mathbf{y}$ given by $p(\mathbf{x},\mathbf{y})$, If the sets of variables are independen, then their joint distribution will factorize into the product of their marginals
 $$p(\mathbf{x},\mathbf{y})=p(\mathbf{x})p(\mathbf{y})$$
If the variables are not independent,we can gain some idea of whether they are ‘close’ to being indepen- dent by considering the Kullback-Leibler divergence between the joint distribution and the product of the marginals, called *mutual information*, given by
\begin{equation}
\label{eq:39}
I[\mathbf{x},\mathbf{y}]=KL(p(\mathbf{x},\mathbf{y})||p(\mathbf{x})p(\mathbf{y}))
\end{equation}
** Relative entropy and mutual information
\begin{eqnarray}
\label{eq:43}
I[\mathbf{x},\mathbf{y}] & = & H[\mathbf{x}]-H[\mathbf{x}|\mathbf{y}]\\
I[\mathbf{x},\mathbf{y}]&  = &H[\mathbf{y}]-H[\mathbf{y}|\mathbf{x}]
\end{eqnarray}
