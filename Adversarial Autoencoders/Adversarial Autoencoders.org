#+TITLE:     Adversarial Autoencoders
#+AUTHOR:    mingzailao
#+EMAIL:     mingzailao@gmail.com
#+DATE:      Tue Dec 13 21:18:14 2016
#+DESCRIPTION: 
#+KEYWORDS: 
#+STARTUP: beamer
#+STARTUP: oddeven
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+BEAMER_THEME: metropolis
#+OPTIONS:   H:2 toc:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)
#+LATEX_HEADER:\def\mathfamilydefault{\rmdefault}
#+BEGIN_EXPORT latex
\AtBeginSection[]
{
\begin{frame}<beamer>
\frametitle{Adversarial Autoencoders}
\tableofcontents[currentsection]
\end{frame}
}
#+END_EXPORT


* Adversarial Autoencoders
** Adversarial Autoencoders

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-13 21:20:37
[[file:Adversarial Autoencoders/screenshot_2016-12-13_21-20-37.png]]

** Adversarial Autoencoders

*** Notations
- $\mathbf{x}$ : input code vector of an autoencoder
- $\mathbf{z}$ : latent code vector of an autoencoder
- $p(\mathbf{z})$ : the prior distribution we want to impose on the codes
- $q(\mathbf{z}|\mathbf{x})$ : an encoding distribution
- $p(\mathbf{x}|\mathbf{z})$ : the decoding distribution
- $p_d(\mathbf{x})$ : the data distribution
- $p(\mathbf{x})$ : the model distribution
** Adversarial Autoencoders
*** aggregated posterior
The encoding function of the autoencoder $q(\mathbf{z}|\mathbf{x})$ defines an aggregated posterior distribution of $q(\mathbf{z})$ on the hidden code vector of the autoencoder as follows :
\begin{eqnarray*}
q(\mathbf{z}) & =& \int_{\mathbf{x}} q(\mathbf{z}|\mathbf{x})p_{d}(\mathbf{x})d\mathbf{x}\\
\end{eqnarray*}
** Adversarial Autoencoders
*** Adversarial Autoencoders
The adversarial autoencoder is an autoencoder that is regularized by matching the aggregated posterior $q(\mathbf{z})$ to an arbitrary prior $p(\mathbf{z})$

- It is the adversarial network that guides $q(\mathbf{z})$ to match $p(\mathbf{z})$.
- The autoencoder, meanwhile, attempts to minimize the reconstruction error.
** Adversarial Autoencoders
*** Notes
- The generator of the adversarial network is also the encoder of the autoencoder $q(\mathbf{z}|\mathbf{x})$.
- The encoder ensures the aggregated posterior distribution can fool the discriminative adversarial network into thinking that the hidden code $q(\mathbf{z})$ comes from the true prior distribution $p(\mathbf{z})$

* Relationship to Variational Autoencoders
** Relationship to Variational Autoencoders
*** VAE
\begin{align*}
&\mathbb{E}_{\mathbf{x}\sim p_d(\mathbf{x})}[-\log p(\mathbf{x})] \\
<& \mathbb{E}_{\mathbf{x}}[-ELBO(q(\mathbf{z}|\mathbf{x}))]\\
 =& \mathbb{E}_{\mathbf{x}}[\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[-\log p(\mathbf{x}|\mathbf{z})]]+\mathbb{E}_{\mathbf{x}}[KL(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))]\\
=&\mathbb{E}_{\mathbf{x}}[\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[-\log p(\mathbf{x}|\mathbf{z})]]-\mathbb{E}_{\mathbf{x}}[H(q(\mathbf{z}|\mathbf{x}))]\\
&+\mathbb{E}_{q(\mathbf{z})}[-\log p(\mathbf{z})]\\
=&\mathbb{E}_{\mathbf{x}}[E_{q(\mathbf{z}|\mathbf{x})}[-\log p(\mathbf{x}|\mathbf{z})]]-\mathbb{E}_{\mathbf{x}}[\sum_i\log \sigma_i(\mathbf{x})]\\
&+\mathbb{E}_{q(\mathbf{z})}[-\log p(\mathbf{z})]+const \\
=&Reconstruction - Entropy + CrossEntropy(q(\mathbf{z}), p(\mathbf{z}))
\end{align*}

** Relationship to Variational Autoencoders
*** Marks
- $q(\mathbf{z})$ : the aggregated posterior ;
- $q(\mathbf{z}|\mathbf{x})$ : Gaussian ;
- $p(\mathbf{z})$ : an arbitrary distribution ;
*** Notes
- first term : the reconstruction term of an autoencoder
- the second and third terms : regularization terms

without regularization tems, this is  standard autoencoder
** Relationship to Variational Autoencoders
*** with regularization terms
the VAE learns a latent representation that is compatible with $p(\mathbf{z})$

- second term : encourages large variances for the posterior distribution;
- third term : minimizes the cross-entropy between the aggregated posterior $q(\mathbf{z})$ and the prior $p(\mathbf{z})$ 

*** The difference
- Replace the second two terms with an adversarial training procedure that encourages $q(\mathbf{z})$ to match to the whole distribution of $p(\mathbf{z})$
** Relationship to Variational Autoencoders

#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-14 14:03:15
[[file:Relationship to Variational Autoencoders/screenshot_2016-12-14_14-03-15.png]]
* Incorporating Label Information in the Adversarial Regularization
** Incorporating Label Information in the Adversarial Regularization


#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-31 21:09:21
[[file:Incorporating Label Information in the Adversarial Regularization/screenshot_2016-12-31_21-09-21.png]]
** Incorporating Label Information in the Adversarial Regularization



#+DOWNLOADED: /tmp/screenshot.png @ 2016-12-31 21:17:19
[[file:Incorporating Label Information in the Adversarial Regularization/screenshot_2016-12-31_21-17-19.png]]
